{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Neural Network tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial will guide you in the process of building a simple feed forward Deep Neural Network (DNN).\n",
    "\n",
    "Our aim is to build a DNN that can serve as a tool to tell apart those events that correspond to the signal (jets produced from top quark decay) from those that correspond to the background (QCD jets). In order to do so, we will train the network's parameters in order to minimize a function called loss $\\mathcal{L}$.\n",
    "\n",
    "As you saw in the previous lecture, several optimization methods are used to help the algorithom in reaching a minimum of the loss $\\mathcal{L}$, we will review the most common ones\n",
    "\n",
    "First let's import some useful tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas, keras\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#get some utils from \n",
    "!git clone https://github.com/zurich-ml/MISiS2019ANN\n",
    "# load the file (on Colab, for local see next block)\n",
    "os.chdir('MISiS2019ANN/')\n",
    "from utils import plot_loss_acc, sel_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# if you run locally, use the following command instead and rerun the whole block afterwards:\n",
    "# %load MISiS2019ANN/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We get our data that is stored in the cloud. Using the link inside a browser, you can also download the data to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "!wget \"https://drive.switch.ch/index.php/s/8vpC3MgmpohQ5tN/download\" -O train.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# alternative download from CERN, uncomment to use\n",
    "# !wget \"https://cernbox.cern.ch/index.php/s/kE1t6E4d7zEOtqp/download?x-access-token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkcm9wX29ubHkiOmZhbHNlLCJleHAiOiIyMDE5LTAyLTI4VDEyOjExOjEyLjU2NjgzMzQwNSswMTowMCIsImV4cGlyZXMiOjAsImlkIjoiMTY1MzUzIiwiaXRlbV90eXBlIjowLCJtdGltZSI6MTU1MTA0MDI3Niwib3duZXIiOiJqZXNjaGxlIiwicGF0aCI6ImVvc2hvbWUtajo0NjQ3NjAxNzc0MDU0NjA0OCIsInByb3RlY3RlZCI6ZmFsc2UsInJlYWRfb25seSI6dHJ1ZSwic2hhcmVfbmFtZSI6InRyYWluLmg1IiwidG9rZW4iOiJrRTF0NkU0ZDd6RU90cXAifQ.d17ucVg_UAneJCKQ-OZvQXdTMdiXZC1Qc6Hd5oDOMDE\" -O train.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring the dataset:\n",
    "\n",
    "## Load the dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "particles_per_event = 40\n",
    "n_events = 10000\n",
    "features = 4\n",
    "\n",
    "# Load the dataset from storage\n",
    "store_train = pandas.HDFStore(\"train.h5\")\n",
    "df_train = store_train.select(\"table\",stop=n_events)\n",
    "\n",
    "# Define a list with the desired kinematic variables to access the dataset\n",
    "cols = [c.format(i) for i in range(particles_per_event) for c in [\"E_{0}\",  \"PX_{0}\",  \"PY_{0}\",  \"PZ_{0}\"]]\n",
    "\n",
    "# Extract the train set and the training labels\n",
    "train = df_train[cols].values[0:n_events].reshape(n_events,particles_per_event,features)\n",
    "train_labels = df_train[\"is_signal_new\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The dataset tensor\n",
    "Our dataset is organized in a rank 3 tensor train, whose elements are labeled by indices (i, j, k). Index i runs on the event number, j numbers the particles in each event and index k numbers the variable associated to the track in the event\n",
    "\n",
    "$$\n",
    "train = (i=1,...,n_{events}, j=1,...,\\textit{particles per event}, k= E_{j} , PX_{j}, PY_{j}, PZ_{j})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Fancy indexing\n",
    "\n",
    "For example, we can access the single particles four momentum in an event by making use of indexing. Let's take the first particle in the first event: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('The first particle in the first event has the following four momentum \\n')\n",
    "print('E_0 = {0:.5g}'.format(train[0,0,0]))\n",
    "print('PX_0 = {0:.5g}'.format(train[0,0,1]))\n",
    "print('PY_0 = {0:.5g}'.format(train[0,0,2]))\n",
    "print('PZ_0 = {0:.5g}'.format(train[0,0,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The same information can be accessed using numpy fancy indexing syntax, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('The first particle in the first event has the following four momentum \\n')\n",
    "print(train[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**: Now, using numpy array methods, calculate the following:\n",
    "\n",
    "1) the mean energy of all particles over all events (hint: exclude from the mean the empty events)\n",
    "\n",
    "\n",
    "2) the mean energy of the first 5 particles for the first 10 events\n",
    "\n",
    "3) the maximum PZ (in modulus) of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Variables at hadron colliders: kinematic invariants\n",
    "\n",
    "\n",
    "The dataset can be cast to the $(E \\text{, } \\phi \\text{, } \\eta)$ space.\n",
    "The rapidity $\\eta$ is related to $PZ$ of the particle track. It is defined as:\n",
    "\n",
    "$$\n",
    "\\eta = \\frac{1}{2} ln \\left(\\frac{E - PZ}{ E + PZ} \\right)\n",
    "$$\n",
    "\n",
    "While $\\phi$ is related to the $(PX, PY)$ components by:\n",
    "\n",
    "$$\n",
    "\\phi = arctg\\left( \\frac{PY}{PX} \\right)\n",
    "$$\n",
    "\n",
    "As shown in the following figures, the CMS detector is formed by:\n",
    "\n",
    "1) a cylindrical barrel detector placed around the interaction point that covers a rapidity range of $|\\eta| < 2.5 $\n",
    "\n",
    "2) two $1.48 < | \\eta | < 3.0$ endcap regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Event Front view             |  Event Side View\n",
    ":-------------------------:|:-------------------------:\n",
    "![Left](imgs/front.png)  |  ![Right](imgs/side.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Plotting events\n",
    "\n",
    "Each event can be displayed as a 2D image in the ($\\phi, \\eta$) coordinates, whose pixels values are the corresponding E recorded by the calorimeter.\n",
    "\n",
    "First we can convert the dataset to this new set of variables. We can do it with a few lines of code thanks to indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "eta = np.where(train[:,:,0]==0, 0, 0.5 * np.log((train[:,:,0] - train[:,:,3]) / (train[:,:,0] + train[:,:,3])))\n",
    "phi = np.where(train[:,:,0]==0, 0, np.arctan(train[:,:,2] / train[:,:,1]))\n",
    "E = train[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then we can check that the $\\phi$ angular coverage is of $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "phi_range = np.abs(phi.min()) + phi.max()\n",
    "print(phi_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And that the rapidity range is within the expected one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "eta_range = np.abs(eta.min()) + eta.max()\n",
    "print(eta.min(), eta.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to draw each event we rescale the dataset to be compatible with the image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "phi_pixels = eta_pixels = 40\n",
    "eta_rescaled = (eta + np.abs(eta.min())) / eta_range * eta_pixels\n",
    "phi_rescaled = (phi + np.abs(phi.min())) / phi_range * phi_pixels\n",
    "\n",
    "pic = np.zeros(shape=(phi_pixels, eta_pixels, 1), dtype=np.float32)\n",
    "pics = np.array([pic for j in range(0, n_events)])\n",
    "    \n",
    "for event in range(n_events):\n",
    "\n",
    "    for n_track in range((phi[event]>0).sum()):\n",
    "        #print(n_track)\n",
    "        phi_coord = int(np.floor(phi_rescaled[event][n_track])) - 1\n",
    "        eta_coord = int(np.floor(eta_rescaled[event][n_track])) - 1\n",
    "            \n",
    "        pics[event, phi_coord, eta_coord] = E[event][n_track]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "event_display = 10\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pics[event_display].reshape(40,40), cmap='viridis')\n",
    "plt.xlabel('eta', fontsize=15)\n",
    "plt.ylabel('phi', fontsize=15)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('E (GeV)')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(E[np.where(2<E)], range=(0,250), bins=50);\n",
    "plt.xlabel('E (GeV)', fontsize=15)\n",
    "plt.ylabel('dN/dE', fontsize=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a dense Deep Neural Network\n",
    "\n",
    "## Adding more layers\n",
    "\n",
    "Visualising the dataset with images helps to familiarize with the dataset, but in the following training we will use the jet constituents representation. Networks dealing with rank 1 inputs are called dense networks.\n",
    "\n",
    "We provide you with the syntax that builds a simple feed forward dense Artificial Neural Network, your task is to make it deep and add 5 layers of 200 nodes each. The syntax for adding one intermediate layer in keras is the following:\n",
    "\n",
    "```python\n",
    "# create the first dense layer\n",
    "dense_layer_1 = keras.layers.Dense(number_of_nodes, activation_function)\n",
    "# add it to the model\n",
    "model.add(dense_layer_1)\n",
    "\n",
    "# this can of course be written equivalently in a shorter form\n",
    "model.add(keras.layers.Dense(number_of_nodes, activation_function))\n",
    "```\n",
    "\n",
    "(The keras documentation site is a useful resource if you're stuck at any time: https://keras.io/)\n",
    "\n",
    "***Exercise***:\n",
    "As an example add 5 layers, each one containing 200 nodes.\n",
    "\n",
    "***Remember***: this is an example and the number of output nodes per layer can be chosen arbitrarily, usally, a good practice is to reduce the number of output nodes with increasing layer number i.e. layer 1 # of output nodes = 200, layer 2 # of output nodes 100 etc.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "# Create the sequential model\n",
    "nodes = 200\n",
    "number_of_classes = 2\n",
    "\n",
    "model = keras.models.Sequential() \n",
    "\n",
    "#########\n",
    "model.add(keras.layers.Dense(nodes,                                          # number of output nodes for layer\n",
    "                             \n",
    "                             input_shape = (particles_per_event*features,),  # important:\n",
    "                                                                             # in the first layer you have to \n",
    "                                                                             # specify the input shape which\n",
    "                                                                             # depends on your data\n",
    "                             \n",
    "                             activation='tanh'))                             # layer's activation function\n",
    "\n",
    "model.add(keras.layers.Dense(number_of_classes,                              # number of classes, in our case 2\n",
    "                                                                             # either signal or background\n",
    "                             \n",
    "                             activation='softmax'))                          # activation of the output layer\n",
    "\n",
    "#########\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy', # Loss used for this model, this gets minimized\n",
    "              \n",
    "              optimizer=\"Adam\",                # Choice of optimizer algorithm\n",
    "              \n",
    "              metrics = [\"accuracy\"])          # a way to measure the performance of your network\n",
    "\n",
    "model_history = model.fit(x=train.reshape(n_events, particles_per_event * features), # train dataset\n",
    "                          \n",
    "                          y=keras.utils.to_categorical(train_labels),                # training labels\n",
    "                          \n",
    "                          validation_split=0.1,   # fraction of the dataset that \n",
    "                                                  # will be used as validation set\n",
    "                          \n",
    "                          batch_size=128,         # fraction of the dataset that the network sees in  \n",
    "                                                  # a cycle of forward and backward propagation\n",
    "                          \n",
    "                          verbose=1,              # set the verbosity during training\n",
    "                                                  # 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "                          \n",
    "                          epochs=300)             # number of times the network sees the entire dataset\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `model.fit()` method outputs a history object whose keys are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the train-validation splitting of the train set, we can see the over-fitting effect of a high capacity network by plotting the accuracy on the training and on the validation set over several epochs and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_history, validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 0) Find the optimal network capacity\n",
    "\n",
    "Deeper networks allow to better approximate more complicated problems, but it may lead to overfitting effects for less complicated problems, in other words many layers and many nodes doesn't always mean better performances.\n",
    "\n",
    "The ***first step*** in building a good network is finding the right architecture, with the right capacity.  \n",
    "In the next exercise we suggest to create a new network with a ***reduced number of nodes***  and ***reduced number of layers*** with respect to the previous example.\n",
    "\n",
    "An ***indication*** to estimate the capacity of the network is the total number of trainable parameters, which you can read off by adding \n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "***Exercise***\n",
    "Find an optimal architecture by reducing the number of nodes and the number of layers. \n",
    "Does it help tackling the overfitting problem? \n",
    "\n",
    "(hint: we suggest to reduce the number of nodes per layer to 10 and reduce the number of layers to 3 intermediate layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "model = keras.models.Sequential() \n",
    "nodes=10\n",
    "number_of_classes=2\n",
    "\n",
    "####\n",
    "\n",
    "#Add layers here\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"Adam\",\n",
    "              metrics = [\"accuracy\"]) \n",
    "\n",
    "model.summary()\n",
    "model_history = model.fit(x=train.reshape(n_events, particles_per_event * features), \n",
    "                          y=keras.utils.to_categorical(train_labels),             \n",
    "                          validation_split=0.1,                             \n",
    "                          batch_size=128,                                     \n",
    "                          verbose=1,                                          \n",
    "                          epochs=300)  # may change that to a lower number to test. Don't forget to increase\n",
    "                                       # again in the end to have a fair comparison with the previous example!\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_history, validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural network optimization methods\n",
    "\n",
    "Sometimes though you may need a network with more trainable parameters and, in order to prevent the overfitting tendency of a high capacity network, several regularization methods are available.\n",
    "\n",
    "These methods are of great importance but must be used with care since they introduce an external bias during the training procedure.\n",
    "\n",
    "## 1) L2 regularization\n",
    "\n",
    "\n",
    "L2 regularization adds a penalty term to the loss which is proportional to the weights' values (L1) or weights' values squared (L2). This prevents some weights to become numerically dominant with respect to others. It consists of appropriately modifying your cost function, from:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  \\hat{y}^{(i)}\\log\\left(y^{(i)}\\right) + (1-\\hat{y}^{(i)})\\log\\left(1- y^{(i)}\\right) \\large{)} \n",
    "$$\n",
    "To:\n",
    "$$\\mathcal{L}_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small \\hat{y}^{(i)}\\log\\left(y^{(i)}\\right) + (1-\\hat{y}^{(i)})\\log\\left(1- y^{(i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "where $\\hat{y}^{(i)}$, ($y^{(i)}$) is the label (network prediction) of the $i$-ith example, $m$ is the total number of examples in the train set and $\\lambda$ is the regularization weight. The index $l$ runs on the layer numbers and $W_{k,j}$ are the weight matrices of the network.\n",
    "\n",
    "In keras, the regularization term are applied on a per-layer basis through the keyword argument.\n",
    "\n",
    "**Syntax**:\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(output_shape,\n",
    "                             kernel_regularizer=kernel_regularizer.l2(lambd),\n",
    "                             bias_regularizer=regularizers.l2(lamb),\n",
    "                             activation_function)\n",
    "```\n",
    "\n",
    "***Exercise*** \n",
    "Revert the network architecture to the high capacity case as in the first example and use it to show the effect of adding L2 regularization on the loss function and on the train/validation accuracies. you can play with the hyperparameter $\\lambda$ (called ```lambd``` in code) and see the effect of stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "# Creates the sequential model\n",
    " \n",
    "lambd = 0.01\n",
    "nodes = 200\n",
    "\n",
    "model_with_reg = keras.models.Sequential()\n",
    "##################\n",
    "\n",
    "\n",
    "#Place layers with L2 regularization here\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model_with_reg.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=[\"accuracy\"]) \n",
    "\n",
    "model_with_reg.summary()\n",
    "\n",
    "# Train the model\n",
    "model_with_reg_history = model_with_reg.fit(x=train.reshape(n_events, particles_per_event * features), \n",
    "                                            y=keras.utils.to_categorical(train_labels), \n",
    "                                            validation_split=0.1, \n",
    "                                            batch_size=128, \n",
    "                                            verbose=1, \n",
    "                                            epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_with_reg_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_with_reg_history, validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Dropout regularization\n",
    "\n",
    "Dropout regularization acts during training, it introduces a non-zero probability of switching some weights' values to zero. By doing this, the network is taught not to rely to heavily on one particular node and helps preventing overfitting.\n",
    "\n",
    "Dropout is added right after the layer's definition you intend to apply it to. For example, after the first layer.\n",
    "\n",
    "**Syntax**\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(nodes, input_shape = (particles_per_event*features,), activation='tanh'))\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "```\n",
    "\n",
    "Note that the ***dropout doesn't add any trainable parameter*** to the network, as you can read directly with the ```model.summary()``` method\n",
    "\n",
    "***Exercise*** Test dropout regularization on the same network and draw the loss/accuracy plots, play with the ```dropout_rate``` to vary the average fraction of input units set to $0$ during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "# Create the sequential model \n",
    "dropout_rate = 0.1\n",
    "nodes = 200\n",
    "\n",
    "model_with_drop = keras.models.Sequential()\n",
    "##################\n",
    "\n",
    "\n",
    "#Place layers with dropout regularization here\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model_with_drop.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"Adam\", \n",
    "              metrics = [\"accuracy\"]) \n",
    "\n",
    "model_with_drop.summary()\n",
    "model_with_drop_history = model_with_drop.fit(x=train.reshape(n_events, particles_per_event * features), \n",
    "                                              y=keras.utils.to_categorical(train_labels), \n",
    "                                              validation_split=0.1, \n",
    "                                              batch_size=128, \n",
    "                                              verbose=1, \n",
    "                                              epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_with_drop_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_with_drop_history, validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3) Adding batch-normalization\n",
    "\n",
    "Batch-normalization (BN) is a useful tool in training since it stabilizes the training process by normalising the weights at each batch to null mean and unit standard deviation. \n",
    "\n",
    "BN is applied to layers singularly and it is generally used to normalise the input to the activation function. It makes the weights of deep layers of the neural network more robust to changes in weights of the first layers. By reducing the coupling between layers it causes each layer to learn more independently from the others and has a net effect of speeding up the learning process.\n",
    "\n",
    "As an example it is implemented for the first layer in the following way\n",
    "\n",
    "**Syntax**: _(notice that it is different from the previously used syntax)_\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(nodes, input_shape=(particles_per_event * features,))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "```\n",
    "In addition we can add a dropout probability with the line:\n",
    "\n",
    "\n",
    "**Syntax**:\n",
    "```python\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "```\n",
    "\n",
    "***Exercise*** The following neural network puts together all the techniques shown in the previous examples, feel free to edit and play with it to see the effect on the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "dropout_rate = 0.1\n",
    "lambd = 0.1\n",
    "nodes = 80\n",
    "\n",
    "model_with_bn = keras.models.Sequential()\n",
    "##################\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes * 2, input_shape=(particles_per_event * features,)))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization())\n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes, kernel_regularizer=regularizers.l2(lambd),\n",
    "                                     bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization())\n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes // 2, # the // is an integer divion\n",
    "                                                 # i.e. 11 // 2 returns the integer 5\n",
    "                                     kernel_regularizer=regularizers.l2(lambd),\n",
    "                                     bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization()) \n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes // 4, kernel_regularizer=regularizers.l2(lambd), \n",
    "                                     bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization()) \n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes // 10, kernel_regularizer=regularizers.l2(lambd),\n",
    "                                     bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization()) \n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model_with_bn.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=\"Adam\", \n",
    "                      metrics = [\"accuracy\"]) \n",
    "\n",
    "model_with_bn.summary()\n",
    "model_with_bn_history = model_with_bn.fit(x=train.reshape(n_events, particles_per_event * features), \n",
    "                                            y=keras.utils.to_categorical(train_labels), \n",
    "                                            validation_split=0.1, \n",
    "                                            batch_size=256, \n",
    "                                            verbose=1, \n",
    "                                            epochs=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_with_bn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(model_with_bn_history, validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating the network performance: ROC/AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Download an independent sample to test\n",
    "!wget \"https://cernbox.cern.ch/index.php/s/lo1JvvMbnPm94Qd/download?x-access-token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkcm9wX29ubHkiOmZhbHNlLCJleHAiOiIyMDE5LTAyLTI4VDEzOjE1OjE1Ljc5MjM0Mzk2OCswMTowMCIsImV4cGlyZXMiOjAsImlkIjoiMTY2MjU1IiwiaXRlbV90eXBlIjowLCJtdGltZSI6MTU1MTM1MjQ5OSwib3duZXIiOiJqZXNjaGxlIiwicGF0aCI6ImVvc2hvbWUtajo0Njk4MzY0NzE3MzAxNzYwMCIsInByb3RlY3RlZCI6ZmFsc2UsInJlYWRfb25seSI6dHJ1ZSwic2hhcmVfbmFtZSI6InRlc3QuaDUiLCJ0b2tlbiI6ImxvMUp2dk1iblBtOTRRZCJ9.itO2b9kms9e98m3EOHdiDKBlaUE2n6COz0rOS9IS7Wg\" -O test.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Evaluate the performance on an independent sample\n",
    "# Prepare input\n",
    "store_test = pandas.HDFStore(\"test.h5\")\n",
    "df_test = store_test.select(\"table\")\n",
    "\n",
    "test_size = 2000\n",
    "\n",
    "test = df_test[cols].values[0:test_size].reshape(test_size, particles_per_event, features)\n",
    "test_labels = keras.utils.to_categorical(df_test[\"is_signal_new\"])[0:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running on test sample. This may take a moment.\")\n",
    "predictions_bn = model_with_bn.predict(test.reshape(test_size, particles_per_event * features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "true_positives = predictions_bn[:,1][np.where(test_labels[:,1]==1)]  #list the NN output values for those events\n",
    "                                                                       #that where actually signal\n",
    "    \n",
    "false_positives = predictions_bn[:,1][np.where(test_labels[:,0]==1)] #list the NN output values for those events\n",
    "                                                                       #that where actually background\n",
    "\n",
    "plt.hist(true_positives, alpha=0.5, bins=80, density=True, label=\"True positives\")\n",
    "plt.hist(false_positives, alpha=0.5, bins=80, density=True, label=\"False positives\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"NN output\", fontsize='15')\n",
    "plt.ylabel(\"Events (a.u.)\", fontsize='15')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "threshold_range = np.linspace(0.0, 1., num=30)      #Creates a list of possible cuts from 0 to 1\n",
    "\n",
    "sig_eps_vals = [sel_eff(true_positives,threshold_range[i]) for i in range(len(threshold_range))]\n",
    "bkg_eps_vals = [sel_eff(false_positives,threshold_range[i]) for i in range(len(threshold_range))]\n",
    "\n",
    "plt.plot(threshold_range, threshold_range, 'black', linestyle='dashed')  # Random choice\n",
    "plt.plot(bkg_eps_vals, sig_eps_vals, 'r', label=\"NN ROC Curve\")          # NN ROC curve\n",
    "pAUC_NN = roc_auc_score(test_labels, predictions_bn)                     # Calculate area under curve\n",
    "\n",
    "plt.xlabel(\"Background rejection efficiency\", fontsize='15')\n",
    "plt.ylabel(\"Signal selection efficiency\", fontsize='15')\n",
    "plt.text(0.69,0.1,\"NN AUC {0:.4g}\\n\".format(pAUC_NN), bbox=dict(boxstyle=\"round\", facecolor='blue', alpha=0.10), \n",
    "         horizontalalignment='center', verticalalignment='center', fontsize='15')\n",
    "\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
